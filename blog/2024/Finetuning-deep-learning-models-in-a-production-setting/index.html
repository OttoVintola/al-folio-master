<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Quick notes on fine-tuning deep learning models | Otto Vintola </title> <meta name="author" content="Otto Vintola"> <meta name="description" content="Some notes about fine-tuning deep learning models from a practical perspective"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ottovintola.github.io/blog/2024/Finetuning-deep-learning-models-in-a-production-setting/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Quick notes on fine-tuning deep learning models",
            "description": "Some notes about fine-tuning deep learning models from a practical perspective",
            "published": "July 31, 2024",
            "authors": [
              
              {
                "author": "Otto Vintola",
                "authorURL": "ottovintola.github.io",
                "affiliations": [
                  {
                    "name": "Aalto University",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Otto</span> Vintola </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/books/">books </a> </li> <li class="nav-item "> <a class="nav-link" href="/photos/">photos </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Quick notes on fine-tuning deep learning models</h1> <p>Some notes about fine-tuning deep learning models from a practical perspective</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#what-is-fine-tuning-exactly">What is fine-tuning exactly</a> </div> <div> <a href="#peft-methods">PEFT methods</a> </div> <ul> <li> <a href="#lora">LoRA</a> </li> <li> <a href="#adapter-modules">Adapter modules</a> </li> </ul> <div> <a href="#hardware-requirements">Hardware requirements</a> </div> <div> <a href="#practical-considerations">Practical considerations</a> </div> <div> <a href="#imbalanced-datasets">Imbalanced datasets</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Fine-tuning is a popular technique to use foundational models in a specialized setting. Most of the models now are a great baseline for business problems, however, most of the issues in data science are related to proprietary data, and not a public dataset – what is the point of using company funds trying to compete against tech giants on <a href="https://www.kaggle.com/datasets/peiyuanliu2001/mmlu-dataset" rel="external nofollow noopener" target="_blank">MMLU</a> – it does not make any sense. However, fine-tuning the out-of-the-box models on proprietary data has shown promising results in solving i.e. NLP or classification tasks. Personally, hubs like <a href="https://huggingface.co/learn" rel="external nofollow noopener" target="_blank">Hugging Face</a> and <a href="https://pytorch.org/hub/" rel="external nofollow noopener" target="_blank">PyTorch hub</a> are such a wonderful place, the idea of the deep learning community coming together to develop and publish state of the art models is great.</p> <h2 id="what-is-fine-tuning">What is fine-tuning</h2> <p>The standard idea of fine-tuning is that the training is continued on a different dataset. If we use \(f\) to denote the model, \(x\) the input data, \(\theta\) the model parameters, \(y\) the target data, and \(\hat{y}\) the predicted data, the training loop can be summarized as follows:</p> <ol> <li>Forward pass \(\hat{y} = f(x; \theta)\)</li> <li>Loss computation (with cross entropy loss) \(\mathcal{L}(y, \hat{y}) = - \sum_{i} y_i \log(\hat{y}_i)\)</li> <li>Backward pass \(\nabla_{\theta} \mathcal{L} = \frac{\partial \mathcal{L}}{\partial \theta}\)</li> <li>Parameter update (using Adam optimizer) \(m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla_{\theta} \mathcal{L}\) <br> \(v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla_{\theta} \mathcal{L})^2\) <br> \(\hat{m}_t = \frac{m_t}{1 - \beta_1^t}\) <br> \(\hat{v}_t = \frac{v_t}{1 - \beta_2^t}\) <br> \(\theta := \theta - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}\) <br> </li> </ol> <p>The code snippet below gives an idea of what fine-tuning looks like in practice.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">'</span><span class="s">base_model.pth</span><span class="sh">'</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">original_training_data</span><span class="p">)</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Original training loop
</span><span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="bp">...</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

<span class="n">fine</span><span class="o">-</span><span class="n">tuning</span><span class="o">-</span><span class="n">data</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">fine_tuning_data</span><span class="p">)</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">fine_tuning_data</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="bp">...</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="sh">'</span><span class="s">fine_tuned_model.pth</span><span class="sh">'</span><span class="p">)</span>


</code></pre></div></div> <h2 id="peft-methods">PEFT methods</h2> <p>The basic forward, backward and parameter update steps are the most basic form of fine-tuning, and for a good reason, it works. However, there are some methods that can be used to speed up the fine-tuning process. One of these methods is the <a href="https://arxiv.org/abs/2106.01345" rel="external nofollow noopener" target="_blank">PEFT</a> method, which is a method for fine-tuning large language models. There will be a brief overview of two PEFT techniques: LoRA and adapter modules.</p> <h1 id="lora">LoRA</h1> <p>The first one is LoRA <d-cite key="hu2021loralowrankadaptationlarge"></d-cite>, which is a low-rank adaptation method for large language models – retraining models becomes less feasible when the number of parameters get higher. The idea is to use a low-rank approximation of the model to reduce the computational cost of fine-tuning. The pre-trained weight matrix \(W_0 \in \mathbb{R}^{d \times k}\) is updated by \(W_0 + \Delta W = W_0 + BA\), where \(B \in \mathbb{R}^{d \times r}\) and \(A \in \mathbb{R}^{r \times k}\) are low-rank matrices and the rank \(r &lt;&lt; min(d, k)\) with \(x \in \mathbb{R}^{k}\). The matrix \(W_0\) does not receive any gradients, and the low-rank matrices are updated using the standard fine-tuning procedure.</p> <figure style="text-align: center;"> <img src="/assets/img/Finetuning/lora.jpeg" alt="A and B are trainable parameters" style="width: 50%; height: auto; display: block; margin: 0 auto;"> <figcaption style="font-style: italic;">A and B are trainable parameters. <d-cite key="hu2021loralowrankadaptationlarge"></d-cite></figcaption> </figure> <p>The time complexity of LoRA is \(O(Ax + B(Ax)) = O(rk + dr) = O(r(k + d))\) since matrix multiplication has a time complexity of \(O(n^3)\) for square matrices. This is a lot faster than the standard fine-tuning procedure which has a time complexity of \(O(dk)\).</p> <p>Using LoRA in <code class="language-plaintext highlighter-rouge">PyTorch</code> requires a redefinition of the model class – which is quite involved but doable. <code class="language-plaintext highlighter-rouge">Hugging Face</code> instead has a <a href="https://huggingface.co/transformers/model_doc/lora.html" rel="external nofollow noopener" target="_blank">LoRA implementation</a> that can be used with a few lines of code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">LoRAForSequenceClassification</span><span class="p">,</span> <span class="n">LoRAConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">LoRAConfig</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">'</span><span class="s">bert-base-uncased</span><span class="sh">'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LoRAForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">'</span><span class="s">bert-base-uncased</span><span class="sh">'</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

</code></pre></div></div> <h2 id="adapter-modules">Adapter modules</h2> <p>The second method is the adapter modules <d-cite key="houlsby2019parameterefficienttransferlearningnlp"></d-cite>, which is a method for fine-tuning large language models. The idea is to add a small set of trainable parameters to the pre-trained model to adapt it to a new task. The adapter modules are added to the pre-trained model and are trained using the standard fine-tuning procedure.</p> <figure style="text-align: center;"> <img src="/assets/img/Finetuning/adapter.jpeg" alt="A and B are trainable parameters" style="width: 50%; height: auto; display: block; margin: 0 auto;"> <figcaption style="font-style: italic;">Adapters added to a Transformer. <d-cite key="houlsby2019parameterefficienttransferlearningnlp"></d-cite> </figcaption> </figure> <p>The following code snippet shows how to add an adapter module to a pre-trained model in <code class="language-plaintext highlighter-rouge">PyTorch</code>. It is relatively simple to implement, it is just like adding another layer on top of the pre-trained model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">torch</span>

<span class="k">class</span> <span class="nc">TransformerWithAdapter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">adapter</span><span class="p">):</span> 
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerWithAdapter</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">'</span><span class="s">base_model.pth</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">adapter</span> <span class="o">=</span> <span class="n">adapter</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transformer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">adapter</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p>Using the adapters embedded to the architecture requires freezing the pre-trained model parameters. This can be done by setting the <code class="language-plaintext highlighter-rouge">requires_grad</code> attribute to <code class="language-plaintext highlighter-rouge">False</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
</code></pre></div></div> <h2 id="hardware-requirements">Hardware requirements</h2> <p>Finetuning might be computationally cheaper than training a foundational model, but it is still not cheap! Finetuning a model with 340 M parameters – like BERT large – then just loading the model to memory requires \textbf{1.3 GB} of space. With quantization, it can be reduced but with basic <code class="language-plaintext highlighter-rouge">torch.float32</code> it is 1.3 GB, because each parameter is a 32-bit (4 byte) number, this means multiplying the number of parameters by 4.</p> <p>Training the same model will require</p> <ol> <li>Model parameters: 340 M parameters * 4 bytes = 1.3 GB</li> <li>Gradients: 340 M parameters * 4 bytes = 1.3 GB</li> <li>Optimizer states: 340 M parameters * 4 bytes = 1.3 GB</li> <li>Activations 340 M parameters * 4 bytes * B = 1.3 GB * B, where B represents the batch size.</li> </ol> <p>This means that the total memory requirement for training the model is 3.9 GB + 1.3 GB * B. Starting from our original 340 M parameters, one would not think that the computational capacity required would explode so suddenly. So, even though fine-tuning is a cheaper and faster alternative to training a foundational model, be prepared for the compute requirements.</p> <h2 id="practical-considerations">Practical considerations</h2> <p>How much data is required to teach a model the proprietary task. Well obviously, this depends on the task, and the model. If the task is complex, then more data is required – especially for edge cases. Considering, how many examples of a specific instance would the foundational model require in the pretraining dataset, can give a rough estimate of how many examples are required in the fine-tuning dataset. For open source models, the number of examples is in the millions, so for a proprietary task, the number of examples should be in the thousands. These are just rough estimates, and the actual number of examples required can be found by experimenting with the model.</p> <h2 id="imbalanced-datasets">Imbalanced datasets</h2> <p>Imbalanced datasets are a common problem in data science, and fine-tuning is no exception. The problem with imbalanced datasets is that the model will learn to predict the majority class, and not the minority class. This can be solved by using a weighted loss function, where the loss of the minority class is weighted more than the majority class. In <code class="language-plaintext highlighter-rouge">PyTorch</code> this can be done i.e. using the <code class="language-plaintext highlighter-rouge">torch.nn.CrossEntropyLoss</code> function, which has a <code class="language-plaintext highlighter-rouge">weight</code> parameter.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="n">weigths</span> <span class="o">=</span> <span class="nf">compute_weights</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span> <span class="o">//</span> <span class="n">weights</span> <span class="n">has</span> <span class="n">to</span> <span class="n">be</span> <span class="n">a</span> <span class="n">tensor</span> <span class="n">of</span> <span class="nf">dim </span><span class="p">(</span><span class="n">num_classes</span><span class="p">)</span>
</code></pre></div></div> <p>Another way to solve the problem is to use a sampler, which samples the minority class more often than the majority class. The idea is to “feed” the model data in a way that the minority class is seen more often than the majority class. For example, with <code class="language-plaintext highlighter-rouge">PyTorch</code> (again) using the <code class="language-plaintext highlighter-rouge">WeightedRandomSampler</code> class.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="n">weights</span> <span class="o">=</span> <span class="nf">compute_weights</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">WeightedRandomSampler</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">weights</span><span class="p">))</span>
<span class="n">data</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">)</span>
</code></pre></div></div> <h2 id="conclusion">Conclusion</h2> <p>Training a foundational model on a sample of proprietary data is a powerful tool to solve a business problem. However, it should not be the only tool in the toolbox, because sometimes it is not the best tool for the job – when you are a hammer, everything looks like a nail. Sometimes being presented with a problem, it is easy to tunnel vision on finetuning when a foundational model is not the best tool for the job. Often times (too often…) traditional software or machine learning models, could also be used. First considering if the data exists, then thinking about the hardware requirements, and time schedule is a good way to start. And additionally, establishing a baseline and goal for the task are crucial.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-07-08-Finetuning.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"OttoVintola/ottovintola.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Otto Vintola. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>