<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://ottovintola.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ottovintola.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-29T13:21:13+00:00</updated><id>https://ottovintola.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Quick notes on fine-tuning deep learning models</title><link href="https://ottovintola.github.io/blog/2024/Finetuning-deep-learning-models-in-a-production-setting/" rel="alternate" type="text/html" title="Quick notes on fine-tuning deep learning models"/><published>2024-07-31T00:00:00+00:00</published><updated>2024-07-31T00:00:00+00:00</updated><id>https://ottovintola.github.io/blog/2024/Finetuning%20deep%20learning%20models%20in%20a%20production%20setting</id><content type="html" xml:base="https://ottovintola.github.io/blog/2024/Finetuning-deep-learning-models-in-a-production-setting/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Fine-tuning is a popular technique to use foundational models in a specialized setting. Most of the models now are a great baseline for business problems, however, most of the issues in data science are related to proprietary data, and not a public dataset – what is the point of using company funds trying to compete against tech giants on <a href="https://www.kaggle.com/datasets/peiyuanliu2001/mmlu-dataset">MMLU</a> – it does not make any sense. However, fine-tuning the out-of-the-box models on proprietary data has shown promising results in solving i.e. NLP or classification tasks. Personally, hubs like <a href="https://huggingface.co/learn">Hugging Face</a> and <a href="https://pytorch.org/hub/">PyTorch hub</a> are such a wonderful place, the idea of the deep learning community coming together to develop and publish state of the art models is great.</p> <h2 id="what-is-fine-tuning">What is fine-tuning</h2> <p>The standard idea of fine-tuning is that the training is continued on a different dataset. If we use \(f\) to denote the model, \(x\) the input data, \(\theta\) the model parameters, \(y\) the target data, and \(\hat{y}\) the predicted data, the training loop can be summarized as follows:</p> <ol> <li>Forward pass \(\hat{y} = f(x; \theta)\)</li> <li>Loss computation (with cross entropy loss) \(\mathcal{L}(y, \hat{y}) = - \sum_{i} y_i \log(\hat{y}_i)\)</li> <li>Backward pass \(\nabla_{\theta} \mathcal{L} = \frac{\partial \mathcal{L}}{\partial \theta}\)</li> <li>Parameter update (using Adam optimizer) \(m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla_{\theta} \mathcal{L}\) <br/> \(v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla_{\theta} \mathcal{L})^2\) <br/> \(\hat{m}_t = \frac{m_t}{1 - \beta_1^t}\) <br/> \(\hat{v}_t = \frac{v_t}{1 - \beta_2^t}\) <br/> \(\theta := \theta - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}\) <br/></li> </ol> <p>The code snippet below gives an idea of what fine-tuning looks like in practice.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">'</span><span class="s">base_model.pth</span><span class="sh">'</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">original_training_data</span><span class="p">)</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Original training loop
</span><span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="bp">...</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

<span class="n">fine</span><span class="o">-</span><span class="n">tuning</span><span class="o">-</span><span class="n">data</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">fine_tuning_data</span><span class="p">)</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">fine_tuning_data</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="bp">...</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="sh">'</span><span class="s">fine_tuned_model.pth</span><span class="sh">'</span><span class="p">)</span>


</code></pre></div></div> <h2 id="peft-methods">PEFT methods</h2> <p>The basic forward, backward and parameter update steps are the most basic form of fine-tuning, and for a good reason, it works. However, there are some methods that can be used to speed up the fine-tuning process. One of these methods is the <a href="https://arxiv.org/abs/2106.01345">PEFT</a> method, which is a method for fine-tuning large language models. There will be a brief overview of two PEFT techniques: LoRA and adapter modules.</p> <h1 id="lora">LoRA</h1> <p>The first one is LoRA <d-cite key="hu2021loralowrankadaptationlarge"></d-cite>, which is a low-rank adaptation method for large language models – retraining models becomes less feasible when the number of parameters get higher. The idea is to use a low-rank approximation of the model to reduce the computational cost of fine-tuning. The pre-trained weight matrix \(W_0 \in \mathbb{R}^{d \times k}\) is updated by \(W_0 + \Delta W = W_0 + BA\), where \(B \in \mathbb{R}^{d \times r}\) and \(A \in \mathbb{R}^{r \times k}\) are low-rank matrices and the rank \(r &lt;&lt; min(d, k)\) with \(x \in \mathbb{R}^{k}\). The matrix \(W_0\) does not receive any gradients, and the low-rank matrices are updated using the standard fine-tuning procedure.</p> <figure style="text-align: center;"> <img src="/assets/img/Finetuning/lora.jpeg" alt="A and B are trainable parameters" style="width: 50%; height: auto; display: block; margin: 0 auto;"/> <figcaption style="font-style: italic;">A and B are trainable parameters. <d-cite key="hu2021loralowrankadaptationlarge"></d-cite></figcaption> </figure> <p>The time complexity of LoRA is \(O(Ax + B(Ax)) = O(rk + dr) = O(r(k + d))\) since matrix multiplication has a time complexity of \(O(n^3)\) for square matrices. This is a lot faster than the standard fine-tuning procedure which has a time complexity of \(O(dk)\).</p> <p>Using LoRA in <code class="language-plaintext highlighter-rouge">PyTorch</code> requires a redefinition of the model class – which is quite involved but doable. <code class="language-plaintext highlighter-rouge">Hugging Face</code> instead has a <a href="https://huggingface.co/transformers/model_doc/lora.html">LoRA implementation</a> that can be used with a few lines of code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">LoRAForSequenceClassification</span><span class="p">,</span> <span class="n">LoRAConfig</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">LoRAConfig</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">'</span><span class="s">bert-base-uncased</span><span class="sh">'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LoRAForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">'</span><span class="s">bert-base-uncased</span><span class="sh">'</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

</code></pre></div></div> <h2 id="adapter-modules">Adapter modules</h2> <p>The second method is the adapter modules <d-cite key="houlsby2019parameterefficienttransferlearningnlp"></d-cite>, which is a method for fine-tuning large language models. The idea is to add a small set of trainable parameters to the pre-trained model to adapt it to a new task. The adapter modules are added to the pre-trained model and are trained using the standard fine-tuning procedure.</p> <figure style="text-align: center;"> <img src="/assets/img/Finetuning/adapter.jpeg" alt="A and B are trainable parameters" style="width: 50%; height: auto; display: block; margin: 0 auto;"/> <figcaption style="font-style: italic;">Adapters added to a Transformer. <d-cite key="houlsby2019parameterefficienttransferlearningnlp"></d-cite> </figcaption> </figure> <p>The following code snippet shows how to add an adapter module to a pre-trained model in <code class="language-plaintext highlighter-rouge">PyTorch</code>. It is relatively simple to implement, it is just like adding another layer on top of the pre-trained model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">torch</span>

<span class="k">class</span> <span class="nc">TransformerWithAdapter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">adapter</span><span class="p">):</span> 
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerWithAdapter</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">'</span><span class="s">base_model.pth</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">adapter</span> <span class="o">=</span> <span class="n">adapter</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transformer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">adapter</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p>Using the adapters embedded to the architecture requires freezing the pre-trained model parameters. This can be done by setting the <code class="language-plaintext highlighter-rouge">requires_grad</code> attribute to <code class="language-plaintext highlighter-rouge">False</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
</code></pre></div></div> <h2 id="hardware-requirements">Hardware requirements</h2> <p>Finetuning might be computationally cheaper than training a foundational model, but it is still not cheap! Finetuning a model with 340 M parameters – like BERT large – then just loading the model to memory requires \textbf{1.3 GB} of space. With quantization, it can be reduced but with basic <code class="language-plaintext highlighter-rouge">torch.float32</code> it is 1.3 GB, because each parameter is a 32-bit (4 byte) number, this means multiplying the number of parameters by 4.</p> <p>Training the same model will require</p> <ol> <li>Model parameters: 340 M parameters * 4 bytes = 1.3 GB</li> <li>Gradients: 340 M parameters * 4 bytes = 1.3 GB</li> <li>Optimizer states: 340 M parameters * 4 bytes = 1.3 GB</li> <li>Activations 340 M parameters * 4 bytes * B = 1.3 GB * B, where B represents the batch size.</li> </ol> <p>This means that the total memory requirement for training the model is 3.9 GB + 1.3 GB * B. Starting from our original 340 M parameters, one would not think that the computational capacity required would explode so suddenly. So, even though fine-tuning is a cheaper and faster alternative to training a foundational model, be prepared for the compute requirements.</p> <h2 id="practical-considerations">Practical considerations</h2> <p>How much data is required to teach a model the proprietary task. Well obviously, this depends on the task, and the model. If the task is complex, then more data is required – especially for edge cases. Considering, how many examples of a specific instance would the foundational model require in the pretraining dataset, can give a rough estimate of how many examples are required in the fine-tuning dataset. For open source models, the number of examples is in the millions, so for a proprietary task, the number of examples should be in the thousands. These are just rough estimates, and the actual number of examples required can be found by experimenting with the model.</p> <h2 id="imbalanced-datasets">Imbalanced datasets</h2> <p>Imbalanced datasets are a common problem in data science, and fine-tuning is no exception. The problem with imbalanced datasets is that the model will learn to predict the majority class, and not the minority class. This can be solved by using a weighted loss function, where the loss of the minority class is weighted more than the majority class. In <code class="language-plaintext highlighter-rouge">PyTorch</code> this can be done i.e. using the <code class="language-plaintext highlighter-rouge">torch.nn.CrossEntropyLoss</code> function, which has a <code class="language-plaintext highlighter-rouge">weight</code> parameter.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="n">weigths</span> <span class="o">=</span> <span class="nf">compute_weights</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span> <span class="o">//</span> <span class="n">weights</span> <span class="n">has</span> <span class="n">to</span> <span class="n">be</span> <span class="n">a</span> <span class="n">tensor</span> <span class="n">of</span> <span class="nf">dim </span><span class="p">(</span><span class="n">num_classes</span><span class="p">)</span>
</code></pre></div></div> <p>Another way to solve the problem is to use a sampler, which samples the minority class more often than the majority class. The idea is to “feed” the model data in a way that the minority class is seen more often than the majority class. For example, with <code class="language-plaintext highlighter-rouge">PyTorch</code> (again) using the <code class="language-plaintext highlighter-rouge">WeightedRandomSampler</code> class.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="n">weights</span> <span class="o">=</span> <span class="nf">compute_weights</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">WeightedRandomSampler</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">weights</span><span class="p">))</span>
<span class="n">data</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">)</span>
</code></pre></div></div> <h2 id="conclusion">Conclusion</h2> <p>Training a foundational model on a sample of proprietary data is a powerful tool to solve a business problem. However, it should not be the only tool in the toolbox, because sometimes it is not the best tool for the job – when you are a hammer, everything looks like a nail. Sometimes being presented with a problem, it is easy to tunnel vision on finetuning when a foundational model is not the best tool for the job. Often times (too often…) traditional software or machine learning models, could also be used. First considering if the data exists, then thinking about the hardware requirements, and time schedule is a good way to start. And additionally, establishing a baseline and goal for the task are crucial.</p>]]></content><author><name>Otto Vintola</name></author><summary type="html"><![CDATA[Some notes about fine-tuning deep learning models from a practical perspective]]></summary></entry><entry><title type="html">Thank you Lukas</title><link href="https://ottovintola.github.io/blog/2024/Programming2/" rel="alternate" type="text/html" title="Thank you Lukas"/><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://ottovintola.github.io/blog/2024/Programming2</id><content type="html" xml:base="https://ottovintola.github.io/blog/2024/Programming2/"><![CDATA[<p>In this course, it was our pleasure and privilege to introduce the students to many of the central principles of computing and programming, to the computer as a machine, and to programming paradigms that enable them to virtualize and scale up their computations all the way to industrial-scale infrastructure, if they so choose.</p> <p>During the course we teach: 1. Bits and data 2. Combinational logic 3. Sequential logic, state and feedback 4. A programmable computer 5. Collections and functions 6. Efficiency 8. Recursion 9. Concurrency and parallelism 10. Machines that learn?</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/programming2/decoder_unit-480.webp 480w,/assets/img/programming2/decoder_unit-800.webp 800w,/assets/img/programming2/decoder_unit-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/programming2/decoder_unit.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/programming2/O-funcs-log-480.webp 480w,/assets/img/programming2/O-funcs-log-800.webp 800w,/assets/img/programming2/O-funcs-log-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/programming2/O-funcs-log.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/programming2/sum-dag-perf-480.webp 480w,/assets/img/programming2/sum-dag-perf-800.webp 800w,/assets/img/programming2/sum-dag-perf-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/programming2/sum-dag-perf.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The course goes in depth into runtimes, computer architecture and the frontiers of computer science. </div> <p>Assignments and an exam is how students are graded. Each week there are weekly programming assignments in Scala about one of the 10 topics listed above. There are also rigorous mathematical proofs and concepts to help the students formalize their learning.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/programming2/concepts-480.webp 480w,/assets/img/programming2/concepts-800.webp 800w,/assets/img/programming2/concepts-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/programming2/concepts.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Description of the Transitivity of big-O notation and some definitions regarding them. </div> <p>Mainly however, the course is about the students learning to think like a computer scientist and express their thoughts in a way that a computer can understand. The assignments are written in Scala with propietary architectures and libraries that we have developed for the course. The course has weekly lectures and labs everyday for almost 8 hours a day.</p> <p>This course is one of the biggest courses in the department and is a requirement for all students in the computer science and data science programs. Every year it has 600 - 800 students. So, you can imagine the scale of the course – during the first lectures there are usually not even enough seats for everyone, so some people end up standing.</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/programming2/csbld-480.webp 480w,/assets/img/programming2/csbld-800.webp 800w,/assets/img/programming2/csbld-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/programming2/csbld.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/programming2/aalto-480.webp 480w,/assets/img/programming2/aalto-800.webp 800w,/assets/img/programming2/aalto-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/programming2/aalto.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Picture of the lecture room in less capacity than the course usually has... </div> <p>Below is an implementation of a <code class="language-plaintext highlighter-rouge">binary search</code> algorithm in Scala, which was used in the course to teach the students about the efficiency of algorithms and the importance of data structures – although there is a separate Data Structures and Algorithms course that runs every autumn semester. It uses an abstract <code class="language-plaintext highlighter-rouge">Ordering[T]</code> to compare arbitrary types <code class="language-plaintext highlighter-rouge">T</code>, however, it is merely a simple example and the real assignments are much more challenging (not to mention the exam).</p> <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">binarySearch</span><span class="o">[</span><span class="kt">T</span><span class="o">](</span><span class="n">s</span><span class="k">:</span> <span class="kt">IndexedSeq</span><span class="o">[</span><span class="kt">T</span><span class="o">],</span> <span class="n">k</span><span class="k">:</span> <span class="kt">T</span><span class="o">)(</span><span class="n">using</span> <span class="nc">Ordering</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span> <span class="k">:</span> <span class="kt">Boolean</span> <span class="o">=</span>
  <span class="k">import</span> <span class="nn">math.Ordered.orderingToOrdered</span> <span class="c1">// To use the given Ordering</span>
  <span class="c1">//require(s.sliding(2).forall(p =&gt; p(0) &lt;= p(1)), "s should be sorted")</span>
  <span class="k">def</span> <span class="nf">inner</span><span class="o">(</span><span class="n">start</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">end</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span>
    <span class="k">if</span> <span class="o">!(</span><span class="n">start</span> <span class="o">&lt;</span> <span class="n">end</span><span class="o">)</span> <span class="n">then</span> <span class="n">start</span>
    <span class="k">else</span>
      <span class="k">val</span> <span class="nv">mid</span> <span class="k">=</span> <span class="o">(</span><span class="n">start</span> <span class="o">+</span> <span class="n">end</span><span class="o">)</span> <span class="o">/</span> <span class="mi">2</span>
      <span class="k">val</span> <span class="nv">cmp</span> <span class="k">=</span> <span class="n">k</span> <span class="n">compare</span> <span class="nf">s</span><span class="o">(</span><span class="n">mid</span><span class="o">)</span>
      <span class="k">if</span> <span class="n">cmp</span> <span class="o">==</span> <span class="mi">0</span> <span class="n">then</span> <span class="n">mid</span>                     <span class="c1">// k == s(mid)</span>
      <span class="k">else</span> <span class="k">if</span> <span class="n">cmp</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="n">then</span> <span class="nf">inner</span><span class="o">(</span><span class="n">start</span><span class="o">,</span> <span class="n">mid</span><span class="o">-</span><span class="mi">1</span><span class="o">)</span> <span class="c1">// k &lt; s(mid)</span>
      <span class="k">else</span> <span class="nf">inner</span><span class="o">(</span><span class="n">mid</span><span class="o">+</span><span class="mi">1</span><span class="o">,</span> <span class="n">end</span><span class="o">)</span>                   <span class="c1">// k &gt; s(mid)</span>
    <span class="n">end</span> <span class="k">if</span>
  <span class="n">end</span> <span class="n">inner</span>
  <span class="k">if</span> <span class="nv">s</span><span class="o">.</span><span class="py">length</span> <span class="o">==</span> <span class="mi">0</span> <span class="n">then</span> <span class="kc">false</span>
  <span class="k">else</span> <span class="nf">s</span><span class="o">(</span><span class="nf">inner</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="nv">s</span><span class="o">.</span><span class="py">length</span><span class="o">-</span><span class="mi">1</span><span class="o">))</span> <span class="o">==</span> <span class="n">k</span>
<span class="n">end</span> <span class="n">binarySearch</span>
</code></pre></div></div> <p>The assignemnts are usually crafted such that online tools and resources can help with collecting information required, but the student has the responsibility to understand and implement the solution by combining the pieces of information they have gathered.</p> <h1 id="references">References</h1> <ol> <li>Programming 2 notes (Petteri Kaski, Tommi Junttila, and Lukas Ahrenberg 2013-2024) at <a href="https://a1120.cs.aalto.fi/2024/notes/">https://a1120.cs.aalto.fi/2024/notes/</a></li> </ol>]]></content><author><name></name></author><summary type="html"><![CDATA[To the best professor in the department and a great person. It was a pleasure to work with you.]]></summary></entry><entry><title type="html">Some recent thoughts</title><link href="https://ottovintola.github.io/blog/2024/What-I-know/" rel="alternate" type="text/html" title="Some recent thoughts"/><published>2024-03-20T00:00:00+00:00</published><updated>2024-03-20T00:00:00+00:00</updated><id>https://ottovintola.github.io/blog/2024/What%20I%20know</id><content type="html" xml:base="https://ottovintola.github.io/blog/2024/What-I-know/"><![CDATA[<div class="row"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/littlebro_and_I-480.webp 480w,/assets/img/littlebro_and_I-800.webp 800w,/assets/img/littlebro_and_I-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/littlebro_and_I.jpeg" class="img-fluid rounded z-depth-1" width="35%" height="35%" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><br/></p> <h2 id="im-an-engineer-not-a-poet">I’m an engineer, not a poet</h2> <p>I love my family – especially the dog. I know I love them because I would do anything for them and I would not let anything bad happen to them. I know they will always be there for me like I will always be there for them –we can always count on each other, there is never a feeling of debt or neglect when we are together.</p> <p><br/></p> <h2 id="i-have-another-confession-to-make">I have another confession to make</h2> <p>Sometimes there are people who fall in love. It is not like falling off a mountain or falling into a river. It is more like a climb to the top of a mountain. I am not talking about crushes or anything like that – usually they are just a feeling of infatuation.</p> <p><br/></p> <h2 id="has-someone-taken-my-faith">Has someone taken my faith</h2> <p>No one knows where they belong and what the future will hold for them. I do not know where I’m headed. I sometimes wonder what its like to be found in someones diary. Everyone has their chains to break. I need someone to help me find my way. I don’t want to keep starting again.</p> <p><br/></p> <h2 id="it-doesnt-really-matter">It doesn’t really matter</h2> <p>In Camus’ book the Stranger the protagonist is indifferent to everything. He does not care about his mother’s death, he does not care about his girlfriend, he does not care about his job. He is indifferent to everything. He is indifferent to life. Personally, I am not a nihilist – life is not whatever. Sitting on the dock and watching it roll by, is not for me. I will make life what I want it to be, however, somethings are out of our control.</p> <p><br/></p> <h2 id="letting-someone-get-the-best-of-you">Letting someone get the best of you</h2> <p>I am too weak to give in. I wish I could give in – to lose. I wish I could let someone behind me. To go to a place where the lost days are just a memory. What do you want to know about me?</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Do not read this]]></summary></entry><entry><title type="html">Introduction to OpenCV using C++ on MacOS</title><link href="https://ottovintola.github.io/blog/2024/introduction-to-opencv-using-c-on-macos/" rel="alternate" type="text/html" title="Introduction to OpenCV using C++ on MacOS"/><published>2024-03-11T13:48:22+00:00</published><updated>2024-03-11T13:48:22+00:00</updated><id>https://ottovintola.github.io/blog/2024/introduction-to-opencv-using-c-on-macos</id><content type="html" xml:base="https://ottovintola.github.io/blog/2024/introduction-to-opencv-using-c-on-macos/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Advanced SQL and Query Optimization</title><link href="https://ottovintola.github.io/blog/2024/Advanced-SQL/" rel="alternate" type="text/html" title="Advanced SQL and Query Optimization"/><published>2024-02-26T11:12:00+00:00</published><updated>2024-02-26T11:12:00+00:00</updated><id>https://ottovintola.github.io/blog/2024/Advanced%20SQL</id><content type="html" xml:base="https://ottovintola.github.io/blog/2024/Advanced-SQL/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Recently, I’ve been studying SQL for some data engineering tasks. I came across a course that mentioned about using Arrays – which I did not even know existed in SQL, additionally, there were ideas and best practices about how to make queries more efficient. I knew that most database systems have <strong>query optimizers</strong> that attempt to interpret/execute the query efficiently, but there were some strategies that could still make a massive impact on the performance of the query.</p> <p><br/></p> <h2 id="nested-data">Nested Data</h2> <p>Lets imagine a hypothetical dataset that has the information about people and their jobs. The data is stored in two tables called <code class="language-plaintext highlighter-rouge">Employees</code> and the columns are <code class="language-plaintext highlighter-rouge">ID</code>, <code class="language-plaintext highlighter-rouge">Name</code>, <code class="language-plaintext highlighter-rouge">Age</code>, and then there is another table called <code class="language-plaintext highlighter-rouge">Information</code> with columns <code class="language-plaintext highlighter-rouge">JobID</code>, <code class="language-plaintext highlighter-rouge">Job</code> and <code class="language-plaintext highlighter-rouge">Tasks</code>. The <code class="language-plaintext highlighter-rouge">JobID</code> is a foreign key that references the <code class="language-plaintext highlighter-rouge">ID</code> in the <code class="language-plaintext highlighter-rouge">people</code> table.</p> <p><strong>Employees Table</strong></p> <table> <thead> <tr> <th style="text-align: left">ID</th> <th style="text-align: left">Name</th> <th style="text-align: left">Age</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">101</td> <td style="text-align: left">John</td> <td style="text-align: left">45</td> </tr> <tr> <td style="text-align: left">42</td> <td style="text-align: left">Jack</td> <td style="text-align: left">35</td> </tr> <tr> <td style="text-align: left">89</td> <td style="text-align: left">Jill</td> <td style="text-align: left">58</td> </tr> </tbody> </table> <p><br/></p> <p><strong>Information Table</strong></p> <table> <thead> <tr> <th style="text-align: left">JobID</th> <th style="text-align: left">Job</th> <th style="text-align: left">Tasks</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">89</td> <td style="text-align: left">Software</td> <td style="text-align: left">Coding</td> </tr> <tr> <td style="text-align: left">42</td> <td style="text-align: left">Sales</td> <td style="text-align: left">Negotiating</td> </tr> <tr> <td style="text-align: left">101</td> <td style="text-align: left">Legal</td> <td style="text-align: left">Advise</td> </tr> </tbody> </table> <p><br/></p> <p>We can either construct the tables like this, or we can use a nested column (a row inside of a row) to store the information. You can think of the row entry being used as a reference to more rows. The nested data structure would look like this:</p> <p><strong>Employees_Information</strong></p> <table> <thead> <tr> <th style="text-align: left">ID</th> <th style="text-align: left">Name</th> <th style="text-align: left">Age</th> <th style="text-align: left">Jobs</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">101</td> <td style="text-align: left">John</td> <td style="text-align: left">45</td> <td style="text-align: left">{Job: Legal, Tasks: Advise}</td> </tr> <tr> <td style="text-align: left">42</td> <td style="text-align: left">Jack</td> <td style="text-align: left">35</td> <td style="text-align: left">{Job: Sales, Tasks: Negotiating}</td> </tr> <tr> <td style="text-align: left">89</td> <td style="text-align: left">Jill</td> <td style="text-align: left">58</td> <td style="text-align: left">{Job: Software, Tasks: Coding}</td> </tr> </tbody> </table> <p><br/> The nested data structure is more efficient because it reduces the number of joins that need to be performed. Additionally, it reduces the database schema complexity making it easier to understand and maintain.</p> <p>To extract the data from this nested structure, we can use a query like this:</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="n">ID</span><span class="p">,</span> <span class="n">Name</span><span class="p">,</span> <span class="n">Age</span>
<span class="k">FROM</span> <span class="n">Employees_Information</span><span class="p">,</span>
    <span class="n">Jobs</span><span class="p">.</span><span class="n">Job</span> <span class="k">AS</span> <span class="n">Job</span><span class="p">,</span> <span class="n">Jobs</span><span class="p">.</span><span class="n">Tasks</span> <span class="k">AS</span> <span class="n">Task</span>
</code></pre></div></div> <p>Which would result in a table like this:</p> <table> <thead> <tr> <th style="text-align: left">ID</th> <th style="text-align: left">Name</th> <th style="text-align: left">Age</th> <th style="text-align: left">Job</th> <th style="text-align: left">Task</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">101</td> <td style="text-align: left">John</td> <td style="text-align: left">45</td> <td style="text-align: left">Legal</td> <td style="text-align: left">Advise</td> </tr> <tr> <td style="text-align: left">42</td> <td style="text-align: left">Jack</td> <td style="text-align: left">35</td> <td style="text-align: left">Sales</td> <td style="text-align: left">Negotiating</td> </tr> <tr> <td style="text-align: left">89</td> <td style="text-align: left">Jill</td> <td style="text-align: left">58</td> <td style="text-align: left">Software</td> <td style="text-align: left">Coding</td> </tr> </tbody> </table> <p><br/></p> <h2 id="repeated-data">Repeated Data</h2> <p>Another a more realistic scenario is when we can have multiple tasks per job. In this scenario we can use an <code class="language-plaintext highlighter-rouge">Array</code> to store the tasks. The <code class="language-plaintext highlighter-rouge">Employees</code> table would look like this:</p> <p><strong>Employees_Information</strong></p> <table> <thead> <tr> <th style="text-align: left">ID</th> <th style="text-align: left">Name</th> <th style="text-align: left">Age</th> <th style="text-align: left">Job</th> <th style="text-align: left">Tasks</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">101</td> <td style="text-align: left">John</td> <td style="text-align: left">45</td> <td style="text-align: left">Legal</td> <td style="text-align: left">[Advise, Research]</td> </tr> <tr> <td style="text-align: left">42</td> <td style="text-align: left">Jack</td> <td style="text-align: left">35</td> <td style="text-align: left">Sales</td> <td style="text-align: left">[Negotiating, Marketing]</td> </tr> <tr> <td style="text-align: left">89</td> <td style="text-align: left">Jill</td> <td style="text-align: left">58</td> <td style="text-align: left">Software</td> <td style="text-align: left">[Coding, Testing]</td> </tr> </tbody> </table> <p><br/> To extract the data from this array structure, we can use a query like this:</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="n">ID</span><span class="p">,</span> <span class="n">Name</span><span class="p">,</span> <span class="n">Age</span><span class="p">,</span> <span class="n">Job</span><span class="p">,</span> <span class="n">Task</span>
<span class="k">FROM</span> <span class="n">Employees_Information</span><span class="p">,</span>
    <span class="k">UNNEST</span><span class="p">(</span><span class="n">Tasks</span><span class="p">)</span> <span class="k">AS</span> <span class="n">Task</span>
</code></pre></div></div> <p>From this query we would get a table like this:</p> <table> <thead> <tr> <th style="text-align: left">ID</th> <th style="text-align: left">Name</th> <th style="text-align: left">Age</th> <th style="text-align: left">Job</th> <th style="text-align: left">Task</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">101</td> <td style="text-align: left">John</td> <td style="text-align: left">45</td> <td style="text-align: left">Legal</td> <td style="text-align: left">Advise</td> </tr> <tr> <td style="text-align: left">101</td> <td style="text-align: left">John</td> <td style="text-align: left">45</td> <td style="text-align: left">Legal</td> <td style="text-align: left">Research</td> </tr> <tr> <td style="text-align: left">42</td> <td style="text-align: left">Jack</td> <td style="text-align: left">35</td> <td style="text-align: left">Sales</td> <td style="text-align: left">Negotiating</td> </tr> <tr> <td style="text-align: left">42</td> <td style="text-align: left">Jack</td> <td style="text-align: left">35</td> <td style="text-align: left">Sales</td> <td style="text-align: left">Marketing</td> </tr> <tr> <td style="text-align: left">89</td> <td style="text-align: left">Jill</td> <td style="text-align: left">58</td> <td style="text-align: left">Software</td> <td style="text-align: left">Coding</td> </tr> <tr> <td style="text-align: left">89</td> <td style="text-align: left">Jill</td> <td style="text-align: left">58</td> <td style="text-align: left">Software</td> <td style="text-align: left">Testing</td> </tr> </tbody> </table> <p><br/> Note the difference thus between using a nested structure and arrays!</p> <p>It is also possible to have nested structures within arrays, and arrays within nested structures. This can be useful for storing complex data structures in a single column.</p> <p><br/></p> <h2 id="query-optimization">Query Optimization</h2> <p>The three main strategies for optimizing queries that I recently learned are</p> <ol> <li>Only select columns needed/wanted</li> <li>Read less data</li> <li>Avoid N:N Joins</li> </ol> <p><br/></p> <h4 id="1-only-select-columns-neededwanted">1. Only select columns needed/wanted</h4> <p>Usually it is tempting to just get everything with <code class="language-plaintext highlighter-rouge">SELECT * FROM ...</code> but this is not efficient if all of the columns are not required for the query/results. To highlight the difference consider these two following queries ran on the <a href="https://cloud.google.com/bigquery/public-data"><code class="language-plaintext highlighter-rouge">bigquery-public-data.github_repos.contents</code></a> table:</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="nv">`bigquery-public-data.github_repos.contents
</span></code></pre></div></div> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="k">size</span><span class="p">,</span> <span class="nb">binary</span> <span class="k">FROM</span> <span class="nv">`bigquery-public-data.github_repos.contents
</span></code></pre></div></div> <p>Data processed: 2682.118 GB <br/> Data processed: 2.531 GB</p> <p><br/></p> <h4 id="2-read-less-data">2. Read less data</h4> <p>This point seems obvious, however, it might not be as straightforward as it seems. The idea is to be parsimonius about what columns to include in the query. Avoid as hard as you can to include columns that are not needed. Well, how can you do that? If there is a column with a 1-to-1 relationship with another column, then you can exclude one of them and use the other. For example, if you have a column <code class="language-plaintext highlighter-rouge">Country</code> and another column <code class="language-plaintext highlighter-rouge">CountryCode</code> you can exclude one of them and this will effectively reduce the amount of data read.</p> <p>Consider the query below:</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="n">Country</span><span class="p">,</span> <span class="n">Population</span><span class="p">,</span> <span class="n">GDP</span><span class="p">,</span> <span class="n">CountryCode</span>
<span class="k">FROM</span> <span class="n">CountryTable</span>
<span class="k">WHERE</span> <span class="n">CountryCode</span> <span class="o">=</span> <span class="s1">'US'</span>
</code></pre></div></div> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="n">Country</span><span class="p">,</span> <span class="n">Population</span><span class="p">,</span> <span class="n">GDP</span>
<span class="k">FROM</span> <span class="n">CountryTable</span>
<span class="k">WHERE</span> <span class="n">Country</span> <span class="o">=</span> <span class="s1">'United States'</span>
</code></pre></div></div> <p>Since, the <code class="language-plaintext highlighter-rouge">CountryCode</code> and <code class="language-plaintext highlighter-rouge">Country</code> have a 1-to-1 correspondence, we can exclude one of them and use the other.</p> <p><br/></p> <h4 id="3-avoid-nn-joins">3. Avoid N:N Joins</h4> <p>The idea here is to avoid joining tables that have a many-to-many relationship and instead separate the queries if possible, and then use an <code class="language-plaintext highlighter-rouge">INNER JOIN</code> to combine the results.</p> <p>The query ran on the <code class="language-plaintext highlighter-rouge">bigquery-public-data.github_repos</code> below shows the difference</p> <p>Slow</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                 <span class="k">SELECT</span> <span class="n">repo</span><span class="p">,</span>
                     <span class="k">COUNT</span><span class="p">(</span><span class="k">DISTINCT</span> <span class="k">c</span><span class="p">.</span><span class="n">committer</span><span class="p">.</span><span class="n">name</span><span class="p">)</span> <span class="k">as</span> <span class="n">num_committers</span><span class="p">,</span>
                     <span class="k">COUNT</span><span class="p">(</span><span class="k">DISTINCT</span> <span class="n">f</span><span class="p">.</span><span class="n">id</span><span class="p">)</span> <span class="k">AS</span> <span class="n">num_files</span>
                 <span class="k">FROM</span> <span class="nv">`bigquery-public-data.github_repos.commits`</span> <span class="k">AS</span> <span class="k">c</span><span class="p">,</span>
                     <span class="k">UNNEST</span><span class="p">(</span><span class="k">c</span><span class="p">.</span><span class="n">repo_name</span><span class="p">)</span> <span class="k">AS</span> <span class="n">repo</span>
                 <span class="k">INNER</span> <span class="k">JOIN</span> <span class="nv">`bigquery-public-data.github_repos.files`</span> <span class="k">AS</span> <span class="n">f</span>
                     <span class="k">ON</span> <span class="n">f</span><span class="p">.</span><span class="n">repo_name</span> <span class="o">=</span> <span class="n">repo</span>
                 <span class="k">WHERE</span> <span class="n">f</span><span class="p">.</span><span class="n">repo_name</span> <span class="k">IN</span> <span class="p">(</span> <span class="s1">'tensorflow/tensorflow'</span><span class="p">,</span> <span class="s1">'facebook/react'</span><span class="p">,</span> <span class="s1">'twbs/bootstrap'</span><span class="p">,</span> <span class="s1">'apple/swift'</span><span class="p">,</span> <span class="s1">'Microsoft/vscode'</span><span class="p">,</span> <span class="s1">'torvalds/linux'</span><span class="p">)</span>
                 <span class="k">GROUP</span> <span class="k">BY</span> <span class="n">repo</span>
                 <span class="k">ORDER</span> <span class="k">BY</span> <span class="n">repo</span>             
</code></pre></div></div> <p>Fast</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code>             <span class="k">WITH</span> <span class="n">commits</span> <span class="k">AS</span>
                   <span class="p">(</span>
                   <span class="k">SELECT</span> <span class="k">COUNT</span><span class="p">(</span><span class="k">DISTINCT</span> <span class="n">committer</span><span class="p">.</span><span class="n">name</span><span class="p">)</span> <span class="k">AS</span> <span class="n">num_committers</span><span class="p">,</span> <span class="n">repo</span>
                   <span class="k">FROM</span> <span class="nv">`bigquery-public-data.github_repos.commits`</span><span class="p">,</span>
                       <span class="k">UNNEST</span><span class="p">(</span><span class="n">repo_name</span><span class="p">)</span> <span class="k">as</span> <span class="n">repo</span>
                   <span class="k">WHERE</span> <span class="n">repo</span> <span class="k">IN</span> <span class="p">(</span> <span class="s1">'tensorflow/tensorflow'</span><span class="p">,</span> <span class="s1">'facebook/react'</span><span class="p">,</span> <span class="s1">'twbs/bootstrap'</span><span class="p">,</span> <span class="s1">'apple/swift'</span><span class="p">,</span> <span class="s1">'Microsoft/vscode'</span><span class="p">,</span> <span class="s1">'torvalds/linux'</span><span class="p">)</span>
                   <span class="k">GROUP</span> <span class="k">BY</span> <span class="n">repo</span>
                   <span class="p">),</span>
                   <span class="n">files</span> <span class="k">AS</span> 
                   <span class="p">(</span>
                   <span class="k">SELECT</span> <span class="k">COUNT</span><span class="p">(</span><span class="k">DISTINCT</span> <span class="n">id</span><span class="p">)</span> <span class="k">AS</span> <span class="n">num_files</span><span class="p">,</span> <span class="n">repo_name</span> <span class="k">as</span> <span class="n">repo</span>
                   <span class="k">FROM</span> <span class="nv">`bigquery-public-data.github_repos.files`</span>
                   <span class="k">WHERE</span> <span class="n">repo_name</span> <span class="k">IN</span> <span class="p">(</span> <span class="s1">'tensorflow/tensorflow'</span><span class="p">,</span> <span class="s1">'facebook/react'</span><span class="p">,</span> <span class="s1">'twbs/bootstrap'</span><span class="p">,</span> <span class="s1">'apple/swift'</span><span class="p">,</span> <span class="s1">'Microsoft/vscode'</span><span class="p">,</span> <span class="s1">'torvalds/linux'</span><span class="p">)</span>
                   <span class="k">GROUP</span> <span class="k">BY</span> <span class="n">repo</span>
                   <span class="p">)</span>
                   <span class="k">SELECT</span> <span class="n">commits</span><span class="p">.</span><span class="n">repo</span><span class="p">,</span> <span class="n">commits</span><span class="p">.</span><span class="n">num_committers</span><span class="p">,</span> <span class="n">files</span><span class="p">.</span><span class="n">num_files</span>
                   <span class="k">FROM</span> <span class="n">commits</span> 
                   <span class="k">INNER</span> <span class="k">JOIN</span> <span class="n">files</span>
                       <span class="k">ON</span> <span class="n">commits</span><span class="p">.</span><span class="n">repo</span> <span class="o">=</span> <span class="n">files</span><span class="p">.</span><span class="n">repo</span>
                   <span class="k">ORDER</span> <span class="k">BY</span> <span class="n">repo</span>      
                   
</code></pre></div></div> <p>The time difference is not significant for this particular query, but if we run it again and again, the difference will add up.</p> <p>Time to run: 13.028 seconds <br/> Time to run: 4.413 seconds <br/><br/></p> <h2 id="conclusion">Conclusion</h2> <p>When I was first learning about SQL, it was in PostgreSQL, which famously does not have arrays built in, so I was quite surprised that they actually exist. Additionally, the idea of nesting data seems logical in order to keep the database schemas more simple – having to do a myriad of <code class="language-plaintext highlighter-rouge">JOIN</code> statements can make queries complex. Lastly, it is also important to track the performance of queries, especially if they are ran on a frequent bases, and attempt to optimize with these principles or just common sense. <br/> <br/> <br/> <br/></p> <h2 id="references">References</h2> <ol> <li><a href="https://www.kaggle.com/learn/advanced-sql">Advanced SQL</a> by Alexis Cook on Kaggle</li> </ol>]]></content><author><name></name></author><summary type="html"><![CDATA[What I learned about SQL and making queries run faster and more sustainably than before]]></summary></entry><entry><title type="html">Multilayer Perceptrons and Backpropagation</title><link href="https://ottovintola.github.io/blog/2024/Multilayer-Perceptrons/" rel="alternate" type="text/html" title="Multilayer Perceptrons and Backpropagation"/><published>2024-02-20T00:00:00+00:00</published><updated>2024-02-20T00:00:00+00:00</updated><id>https://ottovintola.github.io/blog/2024/Multilayer%20Perceptrons</id><content type="html" xml:base="https://ottovintola.github.io/blog/2024/Multilayer-Perceptrons/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>This article will cover the basics about multilayer perceptrons – and the relevant equations to illustrate the inner workings – and how they are trained on data. The goal of this article is not to be a rigorous research paper or a comprehensive guide to the topic, but rather to provide a brief overview of the topic while giving myself the opportunity to recap the important concepts.</p> <p>This will serve as the first addition to the deep learning series, which means there is a small detour to take about deep learning. The basic idea of deep learning will be covered and then we will move on to the main topic of the article. At the end there will be references.</p> <hr/> <h2 id="what-is-deep-learning">What is deep learning?</h2> <p>In a traditional machine learning problem feature engineering would be required. Predicting from the data with human made features, basically the engineer is telling the computer what parts of the data it should use to complete the task. However, sometimes features are hard to specify – especially when dealing with problems that can have many possible options for features such as <strong>image classification</strong> or <strong>natural language processing</strong>.</p> <p>This is where deep learning comes in. <code class="language-plaintext highlighter-rouge">Deep learning is a subset of machine learning where the labels are defined – or learned as we will later see – by the computer itself.</code> <strong>Deep learning</strong> itself is a subset of <strong>representation learning</strong> <d-cite key="bengio2014representation"></d-cite> where we use machine learning to find the mapping from the input to the output and also discover the mapping itself. This means that there is no direct feature engineering required from the engineers, however, providing clean and purposeful data is still crucial – its not possible to just throw in information and expect the wanted results.</p> <p>Sometimes feature engineering is important, like when we want a specified result from a well structured data set, however, deep learning is a powerful tool when dealing with unstructured data. Lets imagine that someone asked you to define the features of an image classification task, what would you say? Maybe its some prominent shape or color. While these may seem natural to a human, they are hard to define to a computer.</p> <p>Deep learning has been around for a long time now – the first neural network was created in 1958 <d-cite key="Rosenblatt1958ThePA"></d-cite> – but it has only recently become popular due to the increase in computational power and the increase in the amount of data available.</p> <hr/> <h2 id="linear-classifier">Linear Classifier</h2> <p>Before we defining multilayer perceptrons we need to understand the linear classifier. Lets say that we had a binary classification problem: our data is \((x^1, y^1) ... (x^n, y^n)\) where \(x^i\) is the input and \(y^i\) is the output where \(x \in R\) and \(y \in \{0, 1\}\). Then with the data the aim is to find a function:</p> \[f(x) = \sigma\left( \sum_{i=1}^{n} w_i x_i + b \right) = \sigma\left(\mathbf{w}^T\mathbf{x} + \mathbf{b}\right)\] <p>where \(\sigma\) is the sigmoid function and \(w_i\) and \(b\) are the weights and bias of the function. The sigmoid function is defined as \(\sigma(x) = \frac{1}{1 + e^{-x}}\).</p> <p>Now with the logistic regression model the output is between 0 and 1, and can be interpreted as the probability that \(x\) belongs to one of the classes \(p(y = 1 \mid x) = f(x)\)</p> <p>At this point, training the classifier would mean finding some \(\textbf{w}\) and \(\textbf{b}\) that would classify most of our examples as correct. This means finding the likelihood function</p> \[p(data | \textbf{w}, \textbf{b}) = \prod_{i=1}^{n} p(y^i | x^i, \textbf{w}, \textbf{b})\] <p>that would maximize the probability of the data given the parameters. This is done by maximizing the log likelihood function (or more commonly minimze the negative of it)</p> \[L(\textbf{w}, \textbf{b}) = - \sum_{i=1}^{n} y^i \log(f(x^i)) + (1 - y^i) \log(1 - f(x^i))\] <p>This loss function specifically is called the cross-entropy loss function and the training of the model is done by minimizing it.</p> <hr/> <h2 id="gradient-descent">Gradient Descent</h2> <p>For now a quick explanation of the minimization process – this will serve as the lead to backpropagation. Most of the training (finding optimal parameters) is done by using the gradient descent algorithm. Which a is a first order optimization algorithm that is used to find the minimum of a function. The algorithm works by taking the derivative of the function at a point and then moving in the opposite direction of the derivative. This is done iteratively until the algorithm converges to a minimum.</p> <p>An image highlights the process quite nicely. The arrows represent the direction of the gradient and the red dot is the minimum of the function. The representation is a contour plot where the lines are orthogonal to the largest gradient. In reality the function is multidimensional and the gradient is a vector, but the idea is the same.</p> <figure style="text-align: center;"> <img src="/assets/img/MLP/GD.png" alt="Gradient Descent" style="width: 50%; height: auto; display: block; margin: 0 auto;"/> <figcaption style="font-style: italic;">Gradient Descent from Wikipedia</figcaption> </figure> <p>The equation representing the iterative process of updating model parameters is</p> \[\theta_{t+1} = \theta_t - \alpha g(\theta_t)\] <p>where \(\theta\) is the parameter, \(\alpha\) is the learning rate and \(g(\theta)\) is the gradient of the function at the point \(\theta\). Now, with backpropagation we can <strong>efficiently</strong> calculate the gradient of the loss function with respect to the parameters of the model and then use gradient descent to update the parameters.</p> <hr/> <h2 id="multilayer-perceptrons">Multilayer Perceptrons</h2> <p>Now, with this information we can define what is a multilayer perceptron (MLP) <d-cite key="Rosenblatt1958ThePA"></d-cite>. They are basically regarded as the general case for a neural network where each neuron implements a function</p> \[f(x) = \phi\left( \sum_{i=1}^{n} w_i x_i + b \right) = \phi\left(\mathbf{w}^T\mathbf{x} + \mathbf{b}\right)\] <p>Where \(\phi\) implements a nonlinear activation function.</p> <p>The idea of the multilayer perceptron is to stack multiple layers of neurons on top of each other. The first layer is the input layer, the last layer is the output layer and the layers in between are called hidden layers. The hidden layers are the ones that make the network deep.</p> <figure style="text-align: center;"> <img src="/assets/img/MLP/MLP.jpg" alt="Fully Connected MLP" style="width: 50%; height: auto; display: block; margin: 0 auto;"/> <figcaption style="font-style: italic;">MLP</figcaption> </figure> <p>Typically, mathematically we represent the network in a more compact style where each node compresses an entire layer.</p> \[h_1 = \phi\left(\mathbf{W}_1^T\mathbf{x} + \mathbf{b}_1\right) \rightarrow h_2 = \phi\left(\mathbf{W}_2^T\mathbf{h}_1 + \mathbf{b}_2\right) \rightarrow y = \phi\left(\mathbf{W}_3^T\mathbf{h}_2 + \mathbf{b}_3\right)\] <p>We can think of the neural network as receiving the input \(x\) and then applying a function \(f(x, \theta)\) where \(\theta\) are the parameters of the model. Our neural network would be \(f_3(f_2(f_1(x, \theta_1), \theta_2), \theta_3)\) where \(f_1, f_2, f_3\) are the functions of the layers and \(\theta_1, \theta_2, \theta_3\) are the parameters of the layers.</p> <p>Now, if the multilayer perceptron is solving a classification problem, the loss function</p> \[L(\theta) = - \sum_{i=1}^{n} y^i \log(f(x^i, \theta)) + (1 - y^i) \log(1 - f(x^i, \theta))\] <p>is used to train the model – note \(\theta\) contains the model parameters \((\textbf{W}, \textbf{b})\). The training is done by using the gradient descent algorithm to minimize the loss function.</p> <hr/> <h2 id="backpropagation">Backpropagation</h2> <p>With massive neural networks calculating gradients becomes computationally expensive and slow. This is where backpropagation comes in. It is a method used to calculate the gradient of the loss function with respect to the parameters and the predicted label. The method is based on the chain rule of calculus and is used to calculate the gradient of the loss function with respect to the parameters of the model.</p> <p>We apply the chain rule to the loss function with respect to each of the parameters</p> \[\frac{\partial L}{\partial \theta} = \sum\frac{\partial L}{\partial y} \frac{\partial y}{\partial \theta}\] \[\frac{\partial L}{\partial h} = \sum\frac{\partial L}{\partial y} \frac{\partial y}{\partial h}\] \[\frac{\partial L}{\partial w} = \sum\frac{\partial L}{\partial h} \frac{\partial h}{\partial w}\] <p>where \(\theta\) are the parameters of the model and \(f\) is the output of the model. The first term is the gradient of the loss function with respect to the output of the model and the second term is the gradient of the output of the model with respect to the parameters of the model. The partial derivatives have to be summed to calculate the effect of the entire layer.</p> <p><code class="language-plaintext highlighter-rouge">From here we can notice that backpropagation relies on the idea that, each connection in the network is affected by the previous layers value propagating to the output layer where the loss was calculated.</code> Recursively calculating derivatives of the loss function with respect to the parameters of the model and the output of the model is the main idea of backpropagation. <strong>Storing the intermediate steps of the calculation is crucial for the efficiency</strong>.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Deep learning has been around for a long time, but it has only recently become popular due to the increase in computational power and the increase in the amount of data available. Deep learning is a subset of machine learning where the labels are defined by the computer itself.</p> <p>In this article we covered the basics about multilayer perceptrons – and the relevant equations – to hopefully shed some light into this bundle of complexity. I got to recap many important topics and ideas as well.</p> <p>I want to take the time to finally thank you (the reader) for reading this article. I hope you found it interesting and that you learned something new. If you have any questions or comments, feel free to email me at <code class="language-plaintext highlighter-rouge">otto.vintola@aalto.fi</code>.</p>]]></content><author><name>Otto Vintola</name></author><summary type="html"><![CDATA[A brief introduction to the theory behind deep learning, linear classifiers, gradient descent and backpropagation explained by going through the necessary information to understand multilayer perceptrons.]]></summary></entry></feed>